

# ðŸ¤– RUC-QA-Reasoning-Until-Convinced


This project implements a sophisticated **Reasoning-based Question Answering** pipeline. Unlike standard RAG systems that provide a single text response, this system builds a **formal argumentation graph** to evaluate multiple hypotheses (answers). It uses local LLMs (via Ollama) and Argumentation Theory to determine the most logically sound answer based on evidence retrieved from Wikipedia.



---

## ðŸ“‚ Project Structure

### ðŸ§  Core Engine (`classes/`)
* **`ServerOllama.py`**: Manages the local connection to the Ollama server and model chat sessions.
* **`LLMUser.py`**: High-level interface for argument extraction, candidate page selection, and **JSON Self-Healing**.
* **`ArgumentationGraph.py`**: The mathematical core. Builds the graph and solves the Quadratic Energy Model using RK4.
* **`PromptBuilder.py`**: Centralized prompt engineering for Wikipedia retrieval and argument mining.
* **`utils.py`**: Utility functions for Wikipedia API fetching and Sentence-BERT ranking (`all-mpnet-base-v2`).
* **`running.py`**: Main orchestrator for running the argumentation logic on specific dataset samples.

### ðŸš€ Execution Pipelines
* **`pipeline_test.py`**: Runs final tests on the `test` split using optimized parameters (`BEST_PARAMS`).
* **`pipeline_val.py`**: Evaluation engine for **Hyperparameter Grid Search** with resume support.
* **`wikipedia.py`**: Knowledge harvester to pre-fetch and cache Wikipedia content into JSON.
* **`baseline.py`**: Runs control group tests (Zero-shot vs Standard RAG) to measure performance gains.


### ðŸš€ Data

* **`results_baseline`**: prediction generated by the baseline  
* **`graph`**: graph generated by the process, can be opened, analyzed, and modified. After that, run the test again to update the performance  
* **`configuration`**: hyperparameters fixed for this


### ðŸ“Š Evaluation & Analysis
* **`evaluation_baseline.py`**: Processes raw prediction logs to calculate filtered accuracy.
* **`evaluation_pipeline.py`**: Analyzes graph performance (Changed vs Unchanged graphs) and generates statistical plots.

---


### ðŸ§ª Pipeline & Evaluation Scripts
These scripts manage the execution of large-scale experiments and data analysis:

1. **`pipeline_test.py` & `pipeline_val.py` (The Executioners)**: 
   - `pipeline_test.py`: Executes final evaluations using the `BEST_PARAMS` identified during search.
   - `pipeline_val.py`: Implements **Grid Search** logic to find the optimal combination of `TOP_PARAGRAPHS` and `MAX_ARGUMENTS`.

2. **`wikipedia.py` (The Knowledge Harvester)**:
   - Specialized utility to build a local knowledge base in `results_wikipedia_only/`. 
   - *Benefit*: Allows multiple experiments without hitting the Wikipedia API repeatedly, increasing speed and reliability.

3. **`baseline.py` (The Control Group)**:
   - Establishes a baseline using **Zero-Shot** (no context) and **Standard RAG** (plain text context).
   - Used to quantify the "accuracy boost" provided by the Argumentation Graph.

4. **`evaluation_baseline.py` & `evaluation_pipeline.py` (The Analysts)**:
   - `evaluation_baseline.py`: Calculates accuracy and filters "INVALID" responses.
   - `evaluation_pipeline.py`: Diagnostic tool comparing questions where the **Graph Changed** vs. remained **Unchanged** (random choice). Generates Matplotlib charts for delta visualization.

---

## ðŸ›  Self-Healing JSON Mechanism
The `LLMUser` class features a robust resilience layer. If a local model produces malformed JSON during extraction, the system:
1. Cleans the output via Regex.
2. Attempts structural repair (fixing brackets).
3. If still invalid, triggers a **Secondary Repair Agent** (e.g., Qwen3) to re-format the data without losing the extracted logic.

---

## ðŸš€ Getting Started

### 1. Prerequisites
Ensure you have **Ollama** installed and the following models pulled:
```bash
ollama pull qwen3:14b
ollama pull gpt-oss:20b
```


### Installation

```bash
git clone [https://github.com/marcocuccarini/RUC-QA-Reasoning-Until-Convinced.git](https://github.com/marcocuccarini/RUC-QA-Reasoning-Until-Convinced.git)
cd RUC-QA-Reasoning-Until-Convinced
pip install -r requirements.txt
```




